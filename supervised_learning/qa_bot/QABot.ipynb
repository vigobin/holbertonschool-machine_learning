{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "Z0mpDzr1tTU3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Uazr8utltNvv"
      },
      "outputs": [],
      "source": [
        "\"\"\"Question Answering\"\"\"\n",
        "\n",
        "def question_answer(question, reference):\n",
        "    \"\"\"Finds a snippet of text within a reference document to answer\n",
        "        a question:\n",
        "        question is a string containing the question to answer.\n",
        "        reference is a string containing the reference document from which to\n",
        "            find the answer.\n",
        "        Returns: a string containing the answer.\n",
        "        If no answer is found, return None.\n",
        "        Your function should use the bert-uncased-tf2-qa model from the\n",
        "            tensorflow-hub library.\n",
        "        Your function should use the pre-trained BertTokenizer,\n",
        "            bert-large-uncased-whole-word-masking-finetuned-squad,\n",
        "            from the transformers library.\"\"\"\n",
        "    model = hub.load('https://tfhub.dev/see--/bert-uncased-tf2-qa/1')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    input_ids = tokenizer.encode(question, reference)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    input_type_ids = [0 if i < input_ids.index(102) else 1\n",
        "                      for i in range(len(input_ids))]\n",
        "\n",
        "    input_ids = tf.constant([input_ids])\n",
        "    input_mask = tf.constant([input_mask])\n",
        "    input_type_ids = tf.constant([input_type_ids])\n",
        "\n",
        "    outputs = model([input_ids, input_mask, input_type_ids])\n",
        "    start_index = tf.argmax(outputs[0][0][1:]) + 1\n",
        "    end_index = tf.argmax(outputs[1][0][1:]) + 1\n",
        "\n",
        "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
        "        input_ids[0][start_index:end_index])\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "    if answer == '[CLS]' or answer == '[SEP]':\n",
        "        return None\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = '/content/drive/MyDrive/Colab Notebooks/data'"
      ],
      "metadata": {
        "id": "CwYNjEB9t5ok"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question_answer = __import__('0-qa').question_answer\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "print(question_answer('When are PLDs?', reference))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0JDoLCFttxz",
        "outputId": "3cbaf57e-4d72-4be7-adf9-f5d483cee5c4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on - site days from 9 : 00 am to 3 : 00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Create the loop\"\"\"\n",
        "\n",
        "while True:\n",
        "    user_input = input('Q: ')\n",
        "    if user_input.lower() in ['exit', 'quit', 'goodbye', 'bye']:\n",
        "        print('A: Goodbye')\n",
        "        break\n",
        "    else:\n",
        "        print('A: ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV7_whyc1J6l",
        "outputId": "c1c23228-ca09-4acf-dd10-d953affd2cc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: a\n",
            "A: \n",
            "Q: hdeohoe\n",
            "A: \n",
            "Q: How\n",
            "A: \n",
            "Q: exit\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Answer Questions\"\"\"\n",
        "\n",
        "def answer_loop(reference):\n",
        "    \"\"\"Answers questions from a reference text:\n",
        "        reference is the reference text.\n",
        "        If the answer cannot be found in the reference text,\n",
        "        respond with Sorry, I do not understand your question.\"\"\"\n",
        "    model = hub.load('https://tfhub.dev/see--/bert-uncased-tf2-qa/1')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    while True:\n",
        "        question = input('Q: ')\n",
        "        if question.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "            print(\"A: Goodbye\")\n",
        "            break\n",
        "\n",
        "        input_ids = tokenizer.encode(question, reference)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        input_type_ids = [0 if i < input_ids.index(102) else 1\n",
        "                        for i in range(len(input_ids))]\n",
        "\n",
        "        input_ids = tf.constant([input_ids])\n",
        "        input_mask = tf.constant([input_mask])\n",
        "        input_type_ids = tf.constant([input_type_ids])\n",
        "\n",
        "        outputs = model([input_ids, input_mask, input_type_ids])\n",
        "        start_index = tf.argmax(outputs[0][0][1:]) + 1\n",
        "        end_index = tf.argmax(outputs[1][0][1:]) + 1\n",
        "\n",
        "        answer_tokens = tokenizer.convert_ids_to_tokens(\n",
        "            input_ids[0][start_index:end_index])\n",
        "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "        if answer:\n",
        "            print('A: ' + answer)\n",
        "        else:\n",
        "            print('A: Sorry, I do not understand your question.')"
      ],
      "metadata": {
        "id": "hGKwdPOm1XHf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer_loop = __import__('2-qa').answer_loop\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "answer_loop(reference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN5IIDy3p7vX",
        "outputId": "f4cc1e36-bf33-4c2b-9bbc-6ccaac63ab2e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: what are mock interviews?\n",
            "A: Sorry, I do not understand your question.\n",
            "Q: what are plds?\n",
            "A: a time for you and your peers to ensure that each of you understands the concepts you ' ve encountered in your projects , as well as a time for everyone to collectively grow in technical , professional , and soft\n",
            "Q: when are plds?\n",
            "A: on - site days from 9 : 00 am to 3 : 00\n",
            "Q: what does PLD stand for?\n",
            "A: peer learning\n",
            "Q: exit\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "nKtC8v-_7-Z7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Semantic Search\"\"\"\n",
        "def semantic_search(corpus_path, sentence):\n",
        "    \"\"\"Performs semantic search on a corpus of documents:\n",
        "        corpus_path is the path to the corpus of reference documents on which\n",
        "            to perform semantic search.\n",
        "        sentence is the sentence from which to perform semantic search.\n",
        "        Returns: the reference text of the document most similar to\n",
        "            sentence.\"\"\"\n",
        "    model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "    embed = hub.load(model_url)\n",
        "\n",
        "    corpus_sentences = []\n",
        "    for filename in os.listdir(corpus_path):\n",
        "        with open(os.path.join(\n",
        "                corpus_path, filename), 'r', encoding='utf-8') as file:\n",
        "            document_text = file.read()\n",
        "            corpus_sentences.append(document_text)\n",
        "\n",
        "    corpus_embeddings = embed(corpus_sentences)\n",
        "    query_embedding = embed([sentence])\n",
        "\n",
        "    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
        "\n",
        "    most_similar_index = similarities.argmax()\n",
        "\n",
        "    most_similar_document_path = os.path.join(corpus_path, os.listdir(\n",
        "         corpus_path)[most_similar_index])\n",
        "\n",
        "    with open(most_similar_document_path, 'r', encoding='utf-8') as file:\n",
        "        most_similar_document_text = file.read()\n",
        "\n",
        "    return most_similar_document_text"
      ],
      "metadata": {
        "id": "1IS_zmmX6w0U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# semantic_search = __import__('3-semantic_search').semantic_search\n",
        "\n",
        "print(semantic_search('ZendeskArticles', 'When are PLDs?'))"
      ],
      "metadata": {
        "id": "aqNk8UGH8BkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Multi-reference Question Answering\"\"\"\n",
        "def question_answer(corpus_path):\n",
        "    \"\"\"Answers questions from multiple reference texts:\n",
        "        corpus_path is the path to the corpus of reference documents.\"\"\"\n",
        "    model = hub.load('https://tfhub.dev/see--/bert-uncased-tf2-qa/1')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    while True:\n",
        "        question = input('Q: ')\n",
        "        if question.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "            print(\"A: Goodbye\")\n",
        "            break\n",
        "\n",
        "        reference = semantic_search(corpus_path, question)\n",
        "        input_ids = tokenizer.encode(question, reference)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        input_type_ids = [0 if i < input_ids.index(102) else 1\n",
        "                        for i in range(len(input_ids))]\n",
        "\n",
        "        input_ids = tf.constant([input_ids])\n",
        "        input_mask = tf.constant([input_mask])\n",
        "        input_type_ids = tf.constant([input_type_ids])\n",
        "\n",
        "        outputs = model([input_ids, input_mask, input_type_ids])\n",
        "        start_index = tf.argmax(outputs[0][0][1:]) + 1\n",
        "        end_index = tf.argmax(outputs[1][0][1:]) + 1\n",
        "\n",
        "        answer_tokens = tokenizer.convert_ids_to_tokens(\n",
        "            input_ids[0][start_index:end_index])\n",
        "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "        if answer:\n",
        "            print('A: ' + answer)\n",
        "        else:\n",
        "            print('A: Sorry, I do not understand your question.')"
      ],
      "metadata": {
        "id": "v1BM0JPVFQHX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question_answer = __import__('4-qa').question_answer\n",
        "\n",
        "question_answer('ZendeskArticles')"
      ],
      "metadata": {
        "id": "ja3EA85qJt9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}