{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "Z0mpDzr1tTU3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uazr8utltNvv"
      },
      "outputs": [],
      "source": [
        "\"\"\"Question Answering\"\"\"\n",
        "\n",
        "def question_answer(question, reference):\n",
        "    \"\"\"Finds a snippet of text within a reference document to answer\n",
        "        a question:\n",
        "        question is a string containing the question to answer.\n",
        "        reference is a string containing the reference document from which to\n",
        "            find the answer.\n",
        "        Returns: a string containing the answer.\n",
        "        If no answer is found, return None.\n",
        "        Your function should use the bert-uncased-tf2-qa model from the\n",
        "            tensorflow-hub library.\n",
        "        Your function should use the pre-trained BertTokenizer,\n",
        "            bert-large-uncased-whole-word-masking-finetuned-squad,\n",
        "            from the transformers library.\"\"\"\n",
        "    model = hub.load('https://tfhub.dev/see--/bert-uncased-tf2-qa/1')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    input_ids = tokenizer.encode(question, reference)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    input_type_ids = [0 if i < input_ids.index(102) else 1\n",
        "                      for i in range(len(input_ids))]\n",
        "\n",
        "    input_ids = tf.constant([input_ids])\n",
        "    input_mask = tf.constant([input_mask])\n",
        "    input_type_ids = tf.constant([input_type_ids])\n",
        "\n",
        "    outputs = model([input_ids, input_mask, input_type_ids])\n",
        "    start_index = tf.argmax(outputs[0][0][1:]) + 1\n",
        "    end_index = tf.argmax(outputs[1][0][1:]) + 1\n",
        "\n",
        "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
        "        input_ids[0][start_index:end_index])\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "    if answer == '[CLS]' or answer == '[SEP]':\n",
        "        return None\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = '/content/drive/MyDrive/Colab Notebooks/data'"
      ],
      "metadata": {
        "id": "CwYNjEB9t5ok"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question_answer = __import__('0-qa').question_answer\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "print(question_answer('When are PLDs?', reference))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0JDoLCFttxz",
        "outputId": "3b48f1a7-d4c6-401a-8a5d-9e2bd5e9d529"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on - site days from 9 : 00 am to 3 : 00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Create the loop\"\"\"\n",
        "\n",
        "while True:\n",
        "    user_input = input('Q: ')\n",
        "    if user_input.lower() in ['exit', 'quit', 'goodbye', 'bye']:\n",
        "        print('A: Goodbye')\n",
        "        break\n",
        "    else:\n",
        "        print('A: ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV7_whyc1J6l",
        "outputId": "c1c23228-ca09-4acf-dd10-d953affd2cc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: a\n",
            "A: \n",
            "Q: hdeohoe\n",
            "A: \n",
            "Q: How\n",
            "A: \n",
            "Q: exit\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Answer Questions\"\"\"\n",
        "\n",
        "def answer_loop(reference):\n",
        "    \"\"\"Answers questions from a reference text:\n",
        "        reference is the reference text.\n",
        "        If the answer cannot be found in the reference text,\n",
        "        respond with Sorry, I do not understand your question.\"\"\"\n",
        "    model = hub.load('https://tfhub.dev/see--/bert-uncased-tf2-qa/1')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    while True:\n",
        "        question = input('Q: ')\n",
        "        if question.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "            print(\"A: Goodbye\")\n",
        "            break\n",
        "\n",
        "        input_ids = tokenizer.encode(question, reference)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        input_type_ids = [0 if i < input_ids.index(102) else 1\n",
        "                        for i in range(len(input_ids))]\n",
        "\n",
        "        input_ids = tf.constant([input_ids])\n",
        "        input_mask = tf.constant([input_mask])\n",
        "        input_type_ids = tf.constant([input_type_ids])\n",
        "\n",
        "        outputs = model([input_ids, input_mask, input_type_ids])\n",
        "        start_index = tf.argmax(outputs[0][0][1:]) + 1\n",
        "        end_index = tf.argmax(outputs[1][0][1:]) + 1\n",
        "\n",
        "        answer_tokens = tokenizer.convert_ids_to_tokens(\n",
        "            input_ids[0][start_index:end_index])\n",
        "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "        if answer:\n",
        "            print('A: ' + answer)\n",
        "        else:\n",
        "            print('A: Sorry, I do not understand your question.')"
      ],
      "metadata": {
        "id": "hGKwdPOm1XHf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer_loop = __import__('2-qa').answer_loop\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/ZendeskArticles/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "answer_loop(reference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN5IIDy3p7vX",
        "outputId": "f4cc1e36-bf33-4c2b-9bbc-6ccaac63ab2e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: what are mock interviews?\n",
            "A: Sorry, I do not understand your question.\n",
            "Q: what are plds?\n",
            "A: a time for you and your peers to ensure that each of you understands the concepts you ' ve encountered in your projects , as well as a time for everyone to collectively grow in technical , professional , and soft\n",
            "Q: when are plds?\n",
            "A: on - site days from 9 : 00 am to 3 : 00\n",
            "Q: what does PLD stand for?\n",
            "A: peer learning\n",
            "Q: exit\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "nKtC8v-_7-Z7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Semantic Search\"\"\"\n",
        "\n",
        "def semantic_search(corpus_path, sentence):\n",
        "    \"\"\"Performs semantic search on a corpus of documents:\n",
        "        corpus_path is the path to the corpus of reference documents on which\n",
        "            to perform semantic search.\n",
        "        sentence is the sentence from which to perform semantic search.\n",
        "        Returns: the reference text of the document most similar to\n",
        "            sentence.\"\"\"\n",
        "    model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "    embed = hub.load(model_url)\n",
        "\n",
        "    corpus_sentences = []\n",
        "    for filename in os.listdir(corpus_path):\n",
        "        if filename.endswith('.md'):\n",
        "            with open(os.path.join(\n",
        "                    corpus_path, filename), 'r', encoding='utf-8') as file:\n",
        "                document_text = file.read()\n",
        "                corpus_sentences.append(document_text)\n",
        "\n",
        "    corpus_embeddings = embed(corpus_sentences)\n",
        "    query_embedding = embed([sentence])\n",
        "\n",
        "    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
        "\n",
        "    most_similar_index = similarities.argmax()\n",
        "\n",
        "    most_similar_document_path = os.path.join(corpus_path, os.listdir(\n",
        "         corpus_path)[most_similar_index])\n",
        "\n",
        "    with open(most_similar_document_path, 'r', encoding='utf-8') as file:\n",
        "        most_similar_document_text = file.read()\n",
        "\n",
        "    return most_similar_document_text"
      ],
      "metadata": {
        "id": "1IS_zmmX6w0U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# semantic_search = __import__('3-semantic_search').semantic_search\n",
        "\n",
        "print(semantic_search('/content/drive/MyDrive/Colab Notebooks/data/ZendeskArticles', 'When are PLDs?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqNk8UGH8BkD",
        "outputId": "2576f4c0-2cb3-419e-8a7e-4fb853788a64"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the beginning of every trimester, each cohort will gather together in small groups for a 30m Roundtable conversation to kick off the trimester together. \n",
            "Objective\n",
            "The Roundtables create small support groups within the cohort and gives the supporting staff an opportunity to get to know the students beyond their technical project-work.\n",
            "Scheduling\n",
            "The Roundtables will be defined by the first PLD groups (with adjustments for absences). The groups meetings will be planned within the first couple of weeks in the trimester. They may span a couple days if there are more groups.\n",
            " \n",
            "Trimester 1\n",
            "Each person will be asked to introduce themselves by:\n",
            "Sharing a bit about their background\n",
            "Offer a way that they can help people\n",
            "Ask for a way they can be supported\n",
            "Trimester 2 & 3\n",
            "Each person will be asked to share:\n",
            "Their experience of the past trimester\n",
            "What their personal goals are for the next trimester (or future)\n",
            "Whether they have new offers or asks of their group\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Multi-reference Question Answering\"\"\"\n",
        "def question_answer(corpus_path):\n",
        "    \"\"\"Answers questions from multiple reference texts:\n",
        "        corpus_path is the path to the corpus of reference documents.\"\"\"\n",
        "    model = hub.load('https://tfhub.dev/see--/bert-uncased-tf2-qa/1')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    while True:\n",
        "        question = input('Q: ')\n",
        "        if question.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "            print(\"A: Goodbye\")\n",
        "            break\n",
        "\n",
        "        reference = semantic_search(corpus_path, question)\n",
        "        input_ids = tokenizer.encode(question, reference)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        input_type_ids = [0 if i < input_ids.index(102) else 1\n",
        "                        for i in range(len(input_ids))]\n",
        "\n",
        "        input_ids = tf.constant([input_ids])\n",
        "        input_mask = tf.constant([input_mask])\n",
        "        input_type_ids = tf.constant([input_type_ids])\n",
        "\n",
        "        outputs = model([input_ids, input_mask, input_type_ids])\n",
        "        start_index = tf.argmax(outputs[0][0][1:]) + 1\n",
        "        end_index = tf.argmax(outputs[1][0][1:]) + 1\n",
        "\n",
        "        answer_tokens = tokenizer.convert_ids_to_tokens(\n",
        "            input_ids[0][start_index:end_index])\n",
        "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "        if answer:\n",
        "            print('A: ' + answer)\n",
        "        else:\n",
        "            print('A: Sorry, I do not understand your question.')"
      ],
      "metadata": {
        "id": "v1BM0JPVFQHX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question_answer = __import__('4-qa').question_answer\n",
        "\n",
        "question_answer('/content/drive/MyDrive/Colab Notebooks/data/ZendeskArticles')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja3EA85qJt9r",
        "outputId": "6b4ec05a-d16d-4c5c-abd4-d4a4810fb740"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: When are PLDs?\n",
            "A: at the beginning of every trim\n",
            "Q: What are Mock Interviews?\n",
            "A: a mock interview has a designated length , each mock interview topic also has its own designated\n",
            "Q: What does PLD stand for?\n",
            "A: peer learning\n",
            "Q: goodbye\n",
            "A: Goodbye\n"
          ]
        }
      ]
    }
  ]
}